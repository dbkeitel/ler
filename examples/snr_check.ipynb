{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "import pycbc\n",
    "from ler import LeR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given: IMR waveform, {self.waveform_approximant}.\n",
      "psds not given. Choosing bilby's default psds\n",
      "given psds:  {'L1': 'aLIGO_O4_high_asd.txt', 'H1': 'aLIGO_O4_high_asd.txt', 'V1': 'AdV_asd.txt'}\n",
      "Interpolator will be loaded for L1 detector from ./interpolator_pickle/L1/halfSNR_dict_2.pickle\n",
      "Interpolator will be loaded for H1 detector from ./interpolator_pickle/H1/halfSNR_dict_2.pickle\n",
      "Interpolator will be loaded for V1 detector from ./interpolator_pickle/V1/halfSNR_dict_2.pickle\n"
     ]
    }
   ],
   "source": [
    "ler = LeR(nsamples_mtot=200,\n",
    "        nsamples_mass_ratio=500,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chosen batch size = 25000. If you want to change batch size, self.batch_size = new_size\n",
      "There will be 4 batche(s)\n",
      "Batch no. 1\n",
      "sampling gw source params...\n",
      "calculating snrs...\n",
      "Batch no. 2\n",
      "sampling gw source params...\n",
      "calculating snrs...\n",
      "Batch no. 3\n",
      "sampling gw source params...\n",
      "calculating snrs...\n",
      "Batch no. 4\n",
      "sampling gw source params...\n",
      "calculating snrs...\n"
     ]
    }
   ],
   "source": [
    "# sampling unlensed events (gw source parameters )\n",
    "#ler.batch_size=50000\n",
    "#ler.unlensed_cbc_statistics(nsamples=1000000, json_file='./gw_params_1M.json');\n",
    "ler.batch_size=25000\n",
    "ler.unlensed_cbc_statistics(nsamples=100000, json_file='./gw_params_100k.json');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting gw_params from json file ./gw_params_100k.json...\n",
      "storing detectable unlensed params in ./gw_params_detectable_100k.json\n",
      "total unlensed rate with step function: 745.8929572230393\n",
      "total unlensed rate with pdet function: 817.7646802329512\n"
     ]
    }
   ],
   "source": [
    "# rates\n",
    "\"\"\"unlensed_rate = ler.unlensed_rate(gw_param='./gw_params_1M.json',\n",
    "    snr_threshold=8.0,\n",
    "    jsonfile='./gw_params_detectable_1M.json',)\"\"\"\n",
    "unlensed_rate = ler.unlensed_rate(gw_param='./gw_params_100k.json',\n",
    "    snr_threshold=8.0,\n",
    "    jsonfile='./gw_params_detectable_100k.json',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chosen batch size = 25000. If you want to change batch size, self.batch_size = new_size\n",
      "There will be 4 batche(s)\n",
      "Batch no. 1\n",
      "sampling lensed params...\n",
      "solving lens equations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 25000/25000 [00:05<00:00, 4701.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating snrs...\n",
      "Batch no. 2\n",
      "sampling lensed params...\n",
      "solving lens equations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 25000/25000 [00:05<00:00, 4627.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating snrs...\n",
      "Batch no. 3\n",
      "sampling lensed params...\n",
      "solving lens equations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 25000/25000 [00:05<00:00, 4534.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating snrs...\n",
      "Batch no. 4\n",
      "sampling lensed params...\n",
      "solving lens equations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 25000/25000 [00:05<00:00, 4705.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating snrs...\n"
     ]
    }
   ],
   "source": [
    "# sampling lens parameters and image(s) parameters\n",
    "lensed_param = ler.lensed_cbc_statistics(nsamples=100000, json_file='./lensed_params_100k.json');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 or more image case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When all image srns are combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read json file for lensed_param\n",
    "with open('./lensed_params_100k.json') as f:\n",
    "    lensed_param = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total rate step:  4.435610020767286\n"
     ]
    }
   ],
   "source": [
    "# Dimensions are (nsamples, n_max_images)\n",
    "snr = np.array(lensed_param[\"opt_snr_net\"])\n",
    "size = len(snr)\n",
    "snr_threshold = 8.\n",
    "\n",
    "# if nan in snr, set snr to 0\n",
    "snr[np.isnan(snr)] = 0.\n",
    "\n",
    "# squared and add up snr in each row, and then take sqrt\n",
    "snr = snr**2\n",
    "snr_sum = np.sum(snr, axis=1)\n",
    "snr_sum = np.sqrt(snr_sum)\n",
    "\n",
    "# boolean array for snr_sum > snr_threshold\n",
    "snr_hit = snr_sum > snr_threshold\n",
    "\n",
    "# rejection sample wrt to weights\n",
    "weights = lensed_param[\"weights\"]\n",
    "not_rejected = np.random.uniform(0, 1, size) < weights\n",
    "snr_hit = snr_hit & not_rejected\n",
    "\n",
    "# calculating rates\n",
    "c0 = ler.lens_galaxy_pop.normalization_pdf_z\n",
    "total_rate_step = c0 * np.mean(snr_hit)\n",
    "\n",
    "print(\"total rate step: \", total_rate_step)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When only two of the snrs are considered seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting lensed_param from json file ./lensed_params_100k.json...\n",
      "storing detectable lensed params in ./lensed_params_detectable_100k.json...\n",
      "total lensed rate with step function: 0.9382357038477389\n",
      "total lensed rate with pdet function: 0.9331544587703197\n"
     ]
    }
   ],
   "source": [
    "# lensed rates\n",
    "lensed_rate = ler.lensed_rate(lensed_param='./lensed_params_100k.json',\n",
    "    jsonfile='./lensed_params_detectable_100k.json',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 image case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When all image srns are combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting 4 image case\n",
    "mag = np.array(lensed_param['magnifications'])\n",
    "snr = np.array(lensed_param[\"opt_snr_net\"])\n",
    "# checking if each row of snr contains nan\n",
    "snr_nan = np.isnan(mag)\n",
    "# sum each row\n",
    "snr_nan_sum = np.sum(snr_nan, axis=1)\n",
    "# boolean array for snr_nan_sum == 0\n",
    "snr_4image = snr_nan_sum == 0\n",
    "\n",
    "# selection\n",
    "snr = snr[snr_4image]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total snr hits:  6502\n"
     ]
    }
   ],
   "source": [
    "# for 100k samples\n",
    "snr_threshold = 8.\n",
    "# squared and add up snr in each row, and then take sqrt\n",
    "snr = snr**2\n",
    "snr_sum = np.sum(snr, axis=1)\n",
    "snr_sum = np.sqrt(snr_sum)\n",
    "\n",
    "# boolean array for snr_sum > snr_threshold\n",
    "snr_hit = snr_sum > snr_threshold\n",
    "\n",
    "# rejection sample wrt to weights\n",
    "weights = np.array(lensed_param[\"weights\"])[snr_4image]\n",
    "not_rejected = np.random.uniform(0, 1, len(weights)) < weights\n",
    "snr_hit = snr_hit & not_rejected\n",
    "\n",
    "print(\"total snr hits: \", np.sum(snr_hit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total snr hits:  64426\n"
     ]
    }
   ],
   "source": [
    "# for 1M samples\n",
    "# squared and add up snr in each row, and then take sqrt\n",
    "\"\"\"snr = snr**2\n",
    "snr_sum = np.sum(snr, axis=1)\n",
    "snr_sum = np.sqrt(snr_sum)\n",
    "\n",
    "# boolean array for snr_sum > snr_threshold\n",
    "snr_hit = snr_sum > snr_threshold\n",
    "\n",
    "# rejection sample wrt to weights\n",
    "weights = lensed_param[\"weights\"][snr_4image]\n",
    "not_rejected = np.random.uniform(0, 1, len(weights)) < weights\n",
    "snr_hit = snr_hit & not_rejected\n",
    "\n",
    "print(\"total snr hits: \", np.sum(snr_hit))\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When only 4 of the snrs are considered seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total snr hits:  553\n"
     ]
    }
   ],
   "source": [
    "# for 100k samples\n",
    "# Dimensions are (nsamples, n_max_images)\n",
    "snr = np.array(lensed_param[\"opt_snr_net\"])\n",
    "snr = np.array(snr[snr_4image])\n",
    "size = len(snr)\n",
    "snr_threshold = 8.\n",
    "\n",
    "# if nan in snr, set snr to 0\n",
    "snr[np.isnan(snr)] = 0.\n",
    "\n",
    "# if the highest and second highest are greater than 8, then set snr as True\n",
    "\"\"\"snr_hit = np.full(len(snr), True)\n",
    "snr = np.sort(snr, axis=1)\n",
    "snr_hit = snr_hit & (snr[:, -1] > snr_threshold) & (snr[:, -2] > snr_threshold)\"\"\"\n",
    "snr_hit = snr>8\n",
    "snr_hit = np.sum(snr_hit,axis=1)\n",
    "\n",
    "# rejection sample wrt to weights\n",
    "weights = np.array(lensed_param[\"weights\"])[snr_4image]\n",
    "not_rejected = np.random.uniform(0, 1, size) < weights\n",
    "snr_hit = snr_hit & not_rejected\n",
    "\n",
    "print(\"total snr hits: \", np.sum(snr_hit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "snr = np.array(lensed_param[\"opt_snr_net\"])\n",
    "snr = np.array(snr[snr_4image])\n",
    "size = len(snr)\n",
    "snr_threshold = 8.\n",
    "\n",
    "# if nan in snr, set snr to 0\n",
    "snr[np.isnan(snr)] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total snr hits:  4582\n"
     ]
    }
   ],
   "source": [
    "# for 1M samples\n",
    "# Dimensions are (nsamples, n_max_images)\n",
    "\"\"\"snr = lensed_param[\"opt_snr_net\"]\n",
    "snr = snr[snr_4image]\n",
    "size = len(snr)\n",
    "snr_threshold = 8.\n",
    "\n",
    "# if nan in snr, set snr to 0\n",
    "snr[np.isnan(snr)] = 0.\n",
    "\n",
    "# if the highest and second highest are greater than 8, then set snr as True\n",
    "#snr_hit = np.full(len(snr), True)\n",
    "#snr = np.sort(snr, axis=1)\n",
    "#snr_hit = snr_hit & (snr[:, -1] > snr_threshold) & (snr[:, -2] > snr_threshold)\n",
    "snr_hit = snr>8\n",
    "snr_hit = np.sum(snr_hit,axis=1)\n",
    "\n",
    "# rejection sample wrt to weights\n",
    "weights = lensed_param[\"weights\"][snr_4image]\n",
    "not_rejected = np.random.uniform(0, 1, size) < weights\n",
    "snr_hit = snr_hit & not_rejected\n",
    "\n",
    "print(\"total snr hits: \", np.sum(snr_hit))\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.29010989010989"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for 100k samples\n",
    "6502/455"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.060672195547795"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for 1M samples\n",
    "64426/4582"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ler",
   "language": "python",
   "name": "ler"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
